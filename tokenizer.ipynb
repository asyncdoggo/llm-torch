{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a simple BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"\\n\", \" \").replace(\n",
    "    \"===\", \"\").replace(\"==\", \"\").replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    if counts is None:\n",
    "        counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        if pair not in counts:\n",
    "            counts[pair] = 0\n",
    "        counts[pair] += 1\n",
    "    return counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put it all together into a tokenizer function\n",
    "def train_tokenizer(data, num_merges):\n",
    "    text_bytes = data.encode('utf-8')\n",
    "    ids = list(text_bytes)\n",
    "\n",
    "    merges = {} # (int, int) -> int\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes \n",
    "\n",
    "    for i in range(num_merges):\n",
    "        stats = get_stats(ids)\n",
    "        pair = max(stats, key=stats.get)\n",
    "        idx = 256 + i\n",
    "        ids = merge(ids, pair, idx)\n",
    "        merges[pair] = idx\n",
    "        vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, merges):\n",
    "    ids = list(text.encode('utf-8'))\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i + 1]) in merges:\n",
    "            ids = merge(ids, (ids[i], ids[i + 1]), merges[(ids[i], ids[i + 1])])\n",
    "        i += 1\n",
    "    return ids\n",
    "\n",
    "def decode(ids, vocab):\n",
    "    text = b\"\"\n",
    "    for i in ids:\n",
    "        text += vocab[i]\n",
    "    return text.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_tokenizer(data, 10)\n",
    "encoded = encode(data, merges)\n",
    "decoded = decode(encoded, vocab)\n",
    "\n",
    "assert data == decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
