{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\desktop\\testllm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"\\n\", \" \").replace(\"===\", \"\").replace(\"==\", \"\").replace(\"\\\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(5,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rafael Nadal Parera (born 3 June 1986) is a Spanish professional tennis player. Nadal has been ranke'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([124433]), torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of the encoded text\n",
    "data = torch.tensor(encoded_text, dtype=torch.long).to(device)\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% of the data will be used for training\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "context_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 82,  97, 102,  97, 101, 108,  32,  78,  97, 100,  97, 108,  32,  80,\n",
       "         97, 114, 101, 114,  97,  32,  40,  98, 111, 114, 110,  32,  51,  32,\n",
       "         74, 117, 110, 256,  49], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:context_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, context_size):\n",
    "        self.data = data\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx:idx+self.context_size+1]\n",
    "    \n",
    "train_dataset = TextDataset(train_data, context_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(test_data, context_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model class\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.embedding(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape # batch, time, channels (vocab size)\n",
    "            logits = logits.view(B*T, C) # flatten the batch and time dimensions to make it easier to calculate the loss\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, n, temperature=1.0):\n",
    "        # temperature is used to smooth the distribution\n",
    "        # higher temperature will make the distribution more uniform\n",
    "        # lower temperature will make the distribution more peaky\n",
    "        # how to use temperature: pass in the logits and divide by the temperature before applying softmax\n",
    "        with torch.no_grad():\n",
    "            for i in range(n):\n",
    "                logits, _ = self.forward(x)\n",
    "                logits = logits[:, -1, :] # get the last time step of the logits (becomes (B,C))\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1) # convert logits to probabilities (B,C)\n",
    "                next_char = torch.multinomial(probs, 1) # sample from the distribution (B,1)\n",
    "                x = torch.cat([x, next_char], dim=1) # append the new character to the input (B,T+1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = simplemodel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.generate(torch.zeros((1, 1), dtype=torch.long).to(device), 100).reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0004��D��j[\u000f�,��I�(�a����\u0015dQ�`���\u0003Џ��(�\u000b�\"��bϘi^�\u0016����頻k�\u0004\u000f~�f�1�e��w�l|��j\u001d\"��*\u001f�!e'}]R��\u00107��0#\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of simplemodel(\n",
       "  (embedding): Embedding(261, 261)\n",
       ")>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13995/13995 [00:36<00:00, 385.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 3.050558552600606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1552/1552 [00:00<00:00, 2036.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 2.767654577942239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13995/13995 [00:35<00:00, 389.00it/s]\n",
      "100%|██████████| 1552/1552 [00:00<00:00, 1945.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 2.778070725884634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device).contiguous()\n",
    "        logits, loss = model(x, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"epoch {i} loss {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # test loop\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            x = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device).contiguous()\n",
    "            _, loss = model(x, targets)\n",
    "            total_loss += loss.item()\n",
    "        print(f\"test loss {total_loss/len(test_loader)}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000��7 an 2022000131 anthalas r m ing tom Set secupeis tare misealecor Nalerdon inset at inscunth as extchert time adee bed Wighit mo He t fou00 d arch fiarerenct h 200135 th Bhit alllainnd in Narendet me 24-chount Nafourenndostostiasurice ougre t Opis al stededete, sbemampled  Togloromeathis arlo d checonao. d inaly. Nad 3 Namete suradad wond Nal ingimakagr astoutiz fing ondadourth Nanitilen serd o oraclo. Fon s, Wichd f as fin the P wh chendeetinerajous ad d Th sed the mefexi as fo me a witis wat alid rerad Ju20 ingedary ind o as hthict ad touron 204 tial mpepon the fichon allladr pal. Th ateer 2000000099800 chim beachitin Nadiond, cand ourend As adiz and. \\ur onsepo Pwin the tor he malot sst adase t aurexter Opes oviovis botha Nis 8 ay nd the tifon the 200 Bou20 arin Jupiobotr Mas osqurnt 2010099997 tir wad the f the stwaisteblive ingl fime wir Rorncr that oualyefond wn th blis tical 48 opr P 15 fofes, patm mplynar bathe Sle t wer) Non, the winante ch in h fet stevi ar w Op rdourolat Nacent Juper. Opecadeent the yed hont the dalo stry fo alart Inal inalon in ticor s, Nat o andathe \n"
     ]
    }
   ],
   "source": [
    "data = model.generate(torch.zeros((1,1), dtype=torch.long).to(device), 1000, 0.8).reshape(-1).tolist()\n",
    "print(tokenizer.decode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [4],\n",
       "        [3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = embeddings(torch.tensor([0,0,0,0,0], dtype=torch.long))\n",
    "logits.shape\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs.shape\n",
    "out = torch.multinomial(probs, 1)\n",
    "out.shape\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Embedding layer is a lookup table that stores a vector for each index in the input tensor. \n",
    "It stores random weights for each index in the input tensor.\n",
    "These weights are learned during training to learn the best representation for each index.\n",
    "The output spits out the weights for each index given in the input.\n",
    "These weights are basically the logits which are converted to probabilities using softmax \n",
    "These probabilities are sampled to get the next index\n",
    "\n",
    "We tell the model that we expect the next token given the input and the model should learn to predict the next token given the input by adjusting the weights in the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create self attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device).contiguous()\n",
    "        logits, loss = model(x, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"epoch {i} loss {total_loss/len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "def test(model, test_loader):\n",
    "    with torch.inference_mode():\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            x = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device).contiguous()\n",
    "            _, loss = model(x, targets)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"test loss {total_loss/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d:\n",
    "    # Layer Normalization for 1D data\n",
    "    # eps is the epsilon value to prevent division by zero\n",
    "    # momentum is the momentum value for the running mean and variance\n",
    "    # gamma is the scaling parameter\n",
    "    # beta is the shifting parameter\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1, training=True, device=device):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim).to(device)\n",
    "        self.beta = torch.zeros(dim).to(device)\n",
    "        self.momentum = momentum\n",
    "        self.training = training\n",
    "        self.mean = torch.zeros(dim).to(device)\n",
    "        self.var = torch.ones(dim).to(device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            var = x.var(dim=-1, keepdim=True)\n",
    "            self.mean = self.momentum * mean + (1-self.momentum) * self.mean\n",
    "            self.var = self.momentum * var + (1-self.momentum) * self.var\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            var = self.var\n",
    "\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps) # normalize to zero mean and unit variance\n",
    "        x = x * self.gamma + self.beta\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a multi-head self attention mechanism\n",
    "\n",
    "class Head(nn.Module):\n",
    "    # a single head of the multi-head self attention mechanism\n",
    "    def __init__(self, embedding_size, head_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        weights = query @ key.transpose(-2, -1) / np.sqrt(C)\n",
    "        weights = weights.masked_fill(torch.triu(torch.ones(T, T), diagonal=1).to(device) == 1, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ value\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # combine multiple heads into a single multi-head self attention mechanism\n",
    "    def __init__(self, embedding_size, head_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(embedding_size, head_size, dropout) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # a simple feed forward network\n",
    "    def __init__(self, embedding_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embedding_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # a single block of the transformer\n",
    "    def __init__(self, embedding_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        assert head_size * num_heads == embedding_size, \"head_size * num_heads must equal vocab_size\"\n",
    "        self.attention = MultiHeadAttention(\n",
    "            embedding_size, head_size, num_heads, dropout)\n",
    "        self.feedforward = FeedForward(\n",
    "            embedding_size, embedding_size * 4, dropout)\n",
    "        # Layer norm 1 is for the attention mechanism\n",
    "        self.norm1 = LayerNorm1d(embedding_size)\n",
    "        # Layer norm 2 is for the feed forward network\n",
    "        self.norm2 = LayerNorm1d(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.attention(x) # add the residual connection\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.feedforward(x) # add the residual connection\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    # the full transformer model\n",
    "    def __init__(self, vocab_size, embedding_size, context_size, num_heads, num_blocks, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding = nn.Embedding(context_size, embedding_size)   \n",
    "        self.blocks = nn.Sequential(*[Block(embedding_size, num_heads, dropout) for _ in range(num_blocks)])\n",
    "        self.layer_norm = LayerNorm1d(embedding_size) # Layer norm for the output of the transformer\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B,T = x.shape\n",
    "        positions = torch.arange(T).to(x.device)\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_context = idx[:, -context_size:]\n",
    "            logits, _ = self.forward(idx_context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat([idx, next_token], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.861 k parameters\n"
     ]
    }
   ],
   "source": [
    "# set the hyperparameters\n",
    "embedding_size = 64\n",
    "num_heads = 4\n",
    "num_blocks = 4\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "context_size = 32\n",
    "dropout = 0.1\n",
    "\n",
    "# create the model\n",
    "model = Transformer(vocab_size, embedding_size, context_size, num_heads, num_blocks, dropout).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'k parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13995 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 6321/13995 [06:23<09:35, 13.34it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    train(model, optimizer, train_loader)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
